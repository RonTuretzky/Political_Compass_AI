{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#| default_exp run\n",
    "#|  export\n",
    "from fastcore.script import call_parse\n",
    "def split_string(string):\n",
    "    # Removing the parentheses and splitting the string by comma\n",
    "    parts = string[1:-1].split(\",\")\n",
    "    # Removing the whitespace and quotes from the parts\n",
    "    parts = [part.strip().strip(\"'\") for part in parts]\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "def return_iters(db:str # Path to db\n",
    "                 ):\n",
    "    train_iter = []\n",
    "    test_iter = []\n",
    "    file = open(db, 'r', encoding='latin1')\n",
    "    mapping = {\n",
    "        \"Libertarian Left\": 1,\n",
    "        \"Libertarian Right\": 2,\n",
    "        \"Authoritarian Left\": 3,\n",
    "        \"Authoritarian Right\": 4,\n",
    "        \"Centrist\": 5,\n",
    "        \"Authoritarian Center\": 6,\n",
    "        \"Left\": 7,\n",
    "        \"Right\": 8,\n",
    "        \"Libertarian Center\": 9,\n",
    "    }\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        opinion,text = split_string(line)\n",
    "        train_iter+=[(mapping[opinion],text)]\n",
    "        test_iter+=[(mapping[opinion],text)]\n",
    "    train_iter = iter(train_iter)\n",
    "    test_iter = iter(test_iter)\n",
    "    file.close()\n",
    "    return train_iter, test_iter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#|  export\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# from Political_Compass_AI.data_processing import return_iters\n",
    "# from Political_Compass_AI.data_processing import split_string\n",
    "from Political_Compass_AI.data_processing import yield_tokens\n",
    "from Political_Compass_AI.data_processing import collate_batch\n",
    "from Political_Compass_AI.model import TextClassificationModel\n",
    "from Political_Compass_AI.training import train\n",
    "from Political_Compass_AI.training import evaluate\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def collate_batch(\n",
    "        batch\n",
    "):\n",
    "    global text_pipeline\n",
    "    global db\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_pipeline = lambda x: int(x) - 1\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "@call_parse\n",
    "def run(\n",
    "    _db:str # dn path to run alignment distribution\n",
    "    ,emsize = 128\n",
    "    ,LR = 5\n",
    "    ,BATCH_SIZE = 32\n",
    "    ,optimizer = \"Adagrad\"\n",
    "    ,EPOCHS = 20\n",
    "\n",
    "\n",
    "):\n",
    "    global text_pipeline\n",
    "    global db\n",
    "    db=_db\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "    label_pipeline = lambda x: int(x) - 1\n",
    "    train_iter, test_iter = return_iters(db)\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    train_iter, test_iter = return_iters(db)\n",
    "    dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "    train_iter, test_iter = return_iters(db)\n",
    "    num_class = len(set([label for (label, text) in train_iter]))\n",
    "    vocab_size = len(vocab)\n",
    "    model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "    run_ledger = open(\"Run_Ledger.txt\", 'a')\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    _optimizer=optimizer\n",
    "    if optimizer==\"Adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=LR)\n",
    "    elif optimizer==\"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "    else:\n",
    "        print(\"Choose a different optimizer\")\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "    total_accu = None\n",
    "    train_iter, test_iter = return_iters(db)\n",
    "    train_dataset = to_map_style_dataset(train_iter)\n",
    "    test_dataset = to_map_style_dataset(test_iter)\n",
    "    num_train = int(len(train_dataset) * 0.95)\n",
    "    split_train_, split_valid_ = \\\n",
    "        random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "    train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True, collate_fn=collate_batch)\n",
    "    valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True, collate_fn=collate_batch)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True, collate_fn=collate_batch)\n",
    "    first_flag = True\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_dataloader, model, optimizer, epoch)\n",
    "        accu_val = evaluate(valid_dataloader, model)\n",
    "        if total_accu is not None and total_accu > accu_val:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            total_accu = accu_val\n",
    "\n",
    "        print('-' * 59)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "              'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                               time.time() - epoch_start_time,\n",
    "                                               accu_val))\n",
    "        print('-' * 59)\n",
    "\n",
    "    accu_test = evaluate(test_dataloader,model)\n",
    "\n",
    "    df_Log = {\"Database_file\":[],\"Epochs\":[],\"LR\":[],\"Batch_Size\":[],\n",
    "              \"Final_accu\":[],\"Optimzer\":[],\"accu_test\":[]}\n",
    "\n",
    "    df_Log[\"Database_file\"].append(db)\n",
    "    df_Log[\"Epochs\"].append(str(EPOCHS))\n",
    "    df_Log[\"LR\"].append( str(LR))\n",
    "    df_Log[\"Batch_Size\"].append(str(BATCH_SIZE))\n",
    "    df_Log[\"Final_accu\"].append(str(accu_val))\n",
    "    df_Log[\"Optimzer\"].append(_optimizer)\n",
    "    df_Log[\"accu_test\"].append(accu_test)\n",
    "\n",
    "    dataframe = pd.DataFrame(df_Log)\n",
    "    dataframe.to_csv('Run_Ledger.csv',mode='a', index=False,sep=\"\\t\")\n",
    "    print(str(accu_test))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "| epoch   1 |    50/ 1463 batches | accuracy    0.124\n",
      "| epoch   1 |   100/ 1463 batches | accuracy    0.138\n",
      "| epoch   1 |   150/ 1463 batches | accuracy    0.121\n",
      "| epoch   1 |   200/ 1463 batches | accuracy    0.160\n",
      "| epoch   1 |   250/ 1463 batches | accuracy    0.142\n",
      "| epoch   1 |   300/ 1463 batches | accuracy    0.155\n",
      "| epoch   1 |   350/ 1463 batches | accuracy    0.138\n",
      "| epoch   1 |   400/ 1463 batches | accuracy    0.138\n",
      "| epoch   1 |   450/ 1463 batches | accuracy    0.142\n",
      "| epoch   1 |   500/ 1463 batches | accuracy    0.149\n",
      "| epoch   1 |   550/ 1463 batches | accuracy    0.136\n",
      "| epoch   1 |   600/ 1463 batches | accuracy    0.148\n",
      "| epoch   1 |   650/ 1463 batches | accuracy    0.144\n",
      "| epoch   1 |   700/ 1463 batches | accuracy    0.134\n",
      "| epoch   1 |   750/ 1463 batches | accuracy    0.165\n",
      "| epoch   1 |   800/ 1463 batches | accuracy    0.154\n",
      "| epoch   1 |   850/ 1463 batches | accuracy    0.156\n",
      "| epoch   1 |   900/ 1463 batches | accuracy    0.149\n",
      "| epoch   1 |   950/ 1463 batches | accuracy    0.144\n",
      "| epoch   1 |  1000/ 1463 batches | accuracy    0.136\n",
      "| epoch   1 |  1050/ 1463 batches | accuracy    0.138\n",
      "| epoch   1 |  1100/ 1463 batches | accuracy    0.164\n",
      "| epoch   1 |  1150/ 1463 batches | accuracy    0.141\n",
      "| epoch   1 |  1200/ 1463 batches | accuracy    0.134\n",
      "| epoch   1 |  1250/ 1463 batches | accuracy    0.156\n",
      "| epoch   1 |  1300/ 1463 batches | accuracy    0.135\n",
      "| epoch   1 |  1350/ 1463 batches | accuracy    0.159\n",
      "| epoch   1 |  1400/ 1463 batches | accuracy    0.144\n",
      "| epoch   1 |  1450/ 1463 batches | accuracy    0.152\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 45.93s | valid accuracy    0.146 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |    50/ 1463 batches | accuracy    0.192\n",
      "| epoch   2 |   100/ 1463 batches | accuracy    0.196\n",
      "| epoch   2 |   150/ 1463 batches | accuracy    0.228\n",
      "| epoch   2 |   200/ 1463 batches | accuracy    0.194\n",
      "| epoch   2 |   250/ 1463 batches | accuracy    0.209\n",
      "| epoch   2 |   300/ 1463 batches | accuracy    0.193\n",
      "| epoch   2 |   350/ 1463 batches | accuracy    0.223\n",
      "| epoch   2 |   400/ 1463 batches | accuracy    0.204\n",
      "| epoch   2 |   450/ 1463 batches | accuracy    0.207\n",
      "| epoch   2 |   500/ 1463 batches | accuracy    0.201\n",
      "| epoch   2 |   550/ 1463 batches | accuracy    0.185\n",
      "| epoch   2 |   600/ 1463 batches | accuracy    0.183\n",
      "| epoch   2 |   650/ 1463 batches | accuracy    0.212\n",
      "| epoch   2 |   700/ 1463 batches | accuracy    0.174\n",
      "| epoch   2 |   750/ 1463 batches | accuracy    0.199\n",
      "| epoch   2 |   800/ 1463 batches | accuracy    0.198\n",
      "| epoch   2 |   850/ 1463 batches | accuracy    0.178\n",
      "| epoch   2 |   900/ 1463 batches | accuracy    0.199\n",
      "| epoch   2 |   950/ 1463 batches | accuracy    0.183\n",
      "| epoch   2 |  1000/ 1463 batches | accuracy    0.191\n",
      "| epoch   2 |  1050/ 1463 batches | accuracy    0.184\n",
      "| epoch   2 |  1100/ 1463 batches | accuracy    0.196\n",
      "| epoch   2 |  1150/ 1463 batches | accuracy    0.187\n",
      "| epoch   2 |  1200/ 1463 batches | accuracy    0.205\n",
      "| epoch   2 |  1250/ 1463 batches | accuracy    0.193\n",
      "| epoch   2 |  1300/ 1463 batches | accuracy    0.193\n",
      "| epoch   2 |  1350/ 1463 batches | accuracy    0.193\n",
      "| epoch   2 |  1400/ 1463 batches | accuracy    0.200\n",
      "| epoch   2 |  1450/ 1463 batches | accuracy    0.198\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 65.81s | valid accuracy    0.146 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |    50/ 1463 batches | accuracy    0.304\n",
      "| epoch   3 |   100/ 1463 batches | accuracy    0.297\n",
      "| epoch   3 |   150/ 1463 batches | accuracy    0.315\n",
      "| epoch   3 |   200/ 1463 batches | accuracy    0.292\n",
      "| epoch   3 |   250/ 1463 batches | accuracy    0.316\n",
      "| epoch   3 |   300/ 1463 batches | accuracy    0.294\n",
      "| epoch   3 |   350/ 1463 batches | accuracy    0.296\n",
      "| epoch   3 |   400/ 1463 batches | accuracy    0.295\n",
      "| epoch   3 |   450/ 1463 batches | accuracy    0.314\n",
      "| epoch   3 |   500/ 1463 batches | accuracy    0.291\n",
      "| epoch   3 |   550/ 1463 batches | accuracy    0.282\n",
      "| epoch   3 |   600/ 1463 batches | accuracy    0.284\n",
      "| epoch   3 |   650/ 1463 batches | accuracy    0.311\n",
      "| epoch   3 |   700/ 1463 batches | accuracy    0.294\n",
      "| epoch   3 |   750/ 1463 batches | accuracy    0.280\n",
      "| epoch   3 |   800/ 1463 batches | accuracy    0.249\n",
      "| epoch   3 |   850/ 1463 batches | accuracy    0.296\n",
      "| epoch   3 |   900/ 1463 batches | accuracy    0.287\n",
      "| epoch   3 |   950/ 1463 batches | accuracy    0.273\n",
      "| epoch   3 |  1000/ 1463 batches | accuracy    0.281\n",
      "| epoch   3 |  1050/ 1463 batches | accuracy    0.269\n",
      "| epoch   3 |  1100/ 1463 batches | accuracy    0.274\n",
      "| epoch   3 |  1150/ 1463 batches | accuracy    0.294\n",
      "| epoch   3 |  1200/ 1463 batches | accuracy    0.268\n",
      "| epoch   3 |  1250/ 1463 batches | accuracy    0.286\n",
      "| epoch   3 |  1300/ 1463 batches | accuracy    0.270\n",
      "| epoch   3 |  1350/ 1463 batches | accuracy    0.301\n",
      "| epoch   3 |  1400/ 1463 batches | accuracy    0.284\n",
      "| epoch   3 |  1450/ 1463 batches | accuracy    0.274\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 81.26s | valid accuracy    0.159 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |    50/ 1463 batches | accuracy    0.425\n",
      "| epoch   4 |   100/ 1463 batches | accuracy    0.446\n",
      "| epoch   4 |   150/ 1463 batches | accuracy    0.408\n",
      "| epoch   4 |   200/ 1463 batches | accuracy    0.405\n",
      "| epoch   4 |   250/ 1463 batches | accuracy    0.418\n",
      "| epoch   4 |   300/ 1463 batches | accuracy    0.418\n",
      "| epoch   4 |   350/ 1463 batches | accuracy    0.377\n",
      "| epoch   4 |   400/ 1463 batches | accuracy    0.392\n",
      "| epoch   4 |   450/ 1463 batches | accuracy    0.398\n",
      "| epoch   4 |   500/ 1463 batches | accuracy    0.395\n",
      "| epoch   4 |   550/ 1463 batches | accuracy    0.403\n",
      "| epoch   4 |   600/ 1463 batches | accuracy    0.393\n",
      "| epoch   4 |   650/ 1463 batches | accuracy    0.394\n",
      "| epoch   4 |   700/ 1463 batches | accuracy    0.355\n",
      "| epoch   4 |   750/ 1463 batches | accuracy    0.392\n",
      "| epoch   4 |   800/ 1463 batches | accuracy    0.381\n",
      "| epoch   4 |   850/ 1463 batches | accuracy    0.383\n",
      "| epoch   4 |   900/ 1463 batches | accuracy    0.382\n",
      "| epoch   4 |   950/ 1463 batches | accuracy    0.418\n",
      "| epoch   4 |  1000/ 1463 batches | accuracy    0.366\n",
      "| epoch   4 |  1050/ 1463 batches | accuracy    0.366\n",
      "| epoch   4 |  1100/ 1463 batches | accuracy    0.341\n",
      "| epoch   4 |  1150/ 1463 batches | accuracy    0.362\n",
      "| epoch   4 |  1200/ 1463 batches | accuracy    0.377\n",
      "| epoch   4 |  1250/ 1463 batches | accuracy    0.381\n",
      "| epoch   4 |  1300/ 1463 batches | accuracy    0.371\n",
      "| epoch   4 |  1350/ 1463 batches | accuracy    0.350\n",
      "| epoch   4 |  1400/ 1463 batches | accuracy    0.376\n",
      "| epoch   4 |  1450/ 1463 batches | accuracy    0.367\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 71.46s | valid accuracy    0.167 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |    50/ 1463 batches | accuracy    0.475\n",
      "| epoch   5 |   100/ 1463 batches | accuracy    0.486\n",
      "| epoch   5 |   150/ 1463 batches | accuracy    0.502\n",
      "| epoch   5 |   200/ 1463 batches | accuracy    0.459\n",
      "| epoch   5 |   250/ 1463 batches | accuracy    0.486\n",
      "| epoch   5 |   300/ 1463 batches | accuracy    0.492\n",
      "| epoch   5 |   350/ 1463 batches | accuracy    0.479\n",
      "| epoch   5 |   400/ 1463 batches | accuracy    0.465\n",
      "| epoch   5 |   450/ 1463 batches | accuracy    0.491\n",
      "| epoch   5 |   500/ 1463 batches | accuracy    0.458\n",
      "| epoch   5 |   550/ 1463 batches | accuracy    0.462\n",
      "| epoch   5 |   600/ 1463 batches | accuracy    0.463\n",
      "| epoch   5 |   650/ 1463 batches | accuracy    0.436\n",
      "| epoch   5 |   700/ 1463 batches | accuracy    0.444\n",
      "| epoch   5 |   750/ 1463 batches | accuracy    0.465\n",
      "| epoch   5 |   800/ 1463 batches | accuracy    0.468\n",
      "| epoch   5 |   850/ 1463 batches | accuracy    0.444\n",
      "| epoch   5 |   900/ 1463 batches | accuracy    0.449\n",
      "| epoch   5 |   950/ 1463 batches | accuracy    0.444\n",
      "| epoch   5 |  1000/ 1463 batches | accuracy    0.446\n",
      "| epoch   5 |  1050/ 1463 batches | accuracy    0.424\n",
      "| epoch   5 |  1100/ 1463 batches | accuracy    0.421\n",
      "| epoch   5 |  1150/ 1463 batches | accuracy    0.403\n",
      "| epoch   5 |  1200/ 1463 batches | accuracy    0.453\n",
      "| epoch   5 |  1250/ 1463 batches | accuracy    0.449\n",
      "| epoch   5 |  1300/ 1463 batches | accuracy    0.436\n",
      "| epoch   5 |  1350/ 1463 batches | accuracy    0.425\n",
      "| epoch   5 |  1400/ 1463 batches | accuracy    0.439\n",
      "| epoch   5 |  1450/ 1463 batches | accuracy    0.417\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 60.86s | valid accuracy    0.156 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |    50/ 1463 batches | accuracy    0.534\n",
      "| epoch   6 |   100/ 1463 batches | accuracy    0.586\n",
      "| epoch   6 |   150/ 1463 batches | accuracy    0.613\n",
      "| epoch   6 |   200/ 1463 batches | accuracy    0.614\n",
      "| epoch   6 |   250/ 1463 batches | accuracy    0.618\n",
      "| epoch   6 |   300/ 1463 batches | accuracy    0.604\n",
      "| epoch   6 |   350/ 1463 batches | accuracy    0.626\n",
      "| epoch   6 |   400/ 1463 batches | accuracy    0.642\n",
      "| epoch   6 |   450/ 1463 batches | accuracy    0.632\n",
      "| epoch   6 |   500/ 1463 batches | accuracy    0.641\n",
      "| epoch   6 |   550/ 1463 batches | accuracy    0.661\n",
      "| epoch   6 |   600/ 1463 batches | accuracy    0.656\n",
      "| epoch   6 |   650/ 1463 batches | accuracy    0.611\n",
      "| epoch   6 |   700/ 1463 batches | accuracy    0.646\n",
      "| epoch   6 |   750/ 1463 batches | accuracy    0.621\n",
      "| epoch   6 |   800/ 1463 batches | accuracy    0.639\n",
      "| epoch   6 |   850/ 1463 batches | accuracy    0.648\n",
      "| epoch   6 |   900/ 1463 batches | accuracy    0.637\n",
      "| epoch   6 |   950/ 1463 batches | accuracy    0.628\n",
      "| epoch   6 |  1000/ 1463 batches | accuracy    0.631\n",
      "| epoch   6 |  1050/ 1463 batches | accuracy    0.653\n",
      "| epoch   6 |  1100/ 1463 batches | accuracy    0.632\n",
      "| epoch   6 |  1150/ 1463 batches | accuracy    0.647\n",
      "| epoch   6 |  1200/ 1463 batches | accuracy    0.643\n",
      "| epoch   6 |  1250/ 1463 batches | accuracy    0.646\n",
      "| epoch   6 |  1300/ 1463 batches | accuracy    0.647\n",
      "| epoch   6 |  1350/ 1463 batches | accuracy    0.626\n"
     ]
    }
   ],
   "source": [
    "model =run(\"../Full_Align_DB\",BATCH_SIZE=32,LR=5,EPOCHS=20)\n",
    "#torch.save(model.state_dict(), <path_to>)\n",
    "# model.load_state_dict(torch.load(<path_to>))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Libertarian Right comment\n"
     ]
    }
   ],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "mapping = {\n",
    "        1:\"Libertarian Left\",\n",
    "        2: \"Libertarian Right\",\n",
    "        3:\"Authoritarian Left\" ,\n",
    "        4: \"Authoritarian Right\",\n",
    "        5: \"Centrist\",\n",
    "         6:\"Authoritarian Center\",\n",
    "        7: \"Left\",\n",
    "        8: \"Right\",\n",
    "        9:\"Libertarian Center\" ,\n",
    "    }\n",
    "model = model.to(\"cpu\")\n",
    "ex_text_str = \"\"\"\n",
    "People didn't even think he was going to invade to begin with, what are you talking about? ðŸ˜…\n",
    "And I have no idea how much European support is carrying Ukraine here vs how effective the Ukrainian resistance is (and how fucking shite the Russian offence is).\n",
    "'Nyhow. Not really relevant. Putin has to be stopped, the war has to end. Aid should be provided to Ukraine. And at the very least, your Socialist Party isn't literally simping for Putin.\n",
    "Which many Communist parties around the world are doing.\n",
    "I just tried to explain why they might do that.\n",
    "Because you called them \"nuts\".\n",
    "There's good reasons not to trust NATO or the U.S. I hope time won't prove me correct, but it is naÃ¯ve to assume that America's already hyper-conservative neoliberal approach cannot slide further right.\n",
    "For the record, I also don't think the U.S is doing anything inherently bad in terms of helping Ukraine (unlike what someone like Jimmy Dore might spin).\n",
    "\"\"\"\n",
    "print(\"This is a %s comment\" % mapping[predict(ex_text_str, text_pipeline)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
