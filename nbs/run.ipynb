{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "#| default_exp run\n",
    "#|  export\n",
    "from fastcore.script import call_parse\n",
    "def split_string(string):\n",
    "    # Removing the parentheses and splitting the string by comma\n",
    "    parts = string[1:-1].split(\",\")\n",
    "    # Removing the whitespace and quotes from the parts\n",
    "    parts = [part.strip().strip(\"'\") for part in parts]\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "def return_iters(db:str # Path to db\n",
    "                 ):\n",
    "    train_iter = []\n",
    "    test_iter = []\n",
    "    file = open(db, 'r', encoding='latin1')\n",
    "    mapping = {\n",
    "        \"Libertarian Left\": 1,\n",
    "        \"Libertarian Right\": 2,\n",
    "        \"Authoritarian Left\": 3,\n",
    "        \"Authoritarian Right\": 4,\n",
    "        \"Centrist\": 5,\n",
    "        \"Authoritarian Center\": 6,\n",
    "        \"Left\": 7,\n",
    "        \"Right\": 8,\n",
    "        \"Libertarian Center\": 9,\n",
    "    }\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        opinion,text = split_string(line)\n",
    "        train_iter+=[(mapping[opinion],text)]\n",
    "        test_iter+=[(mapping[opinion],text)]\n",
    "    train_iter = iter(train_iter)\n",
    "    test_iter = iter(test_iter)\n",
    "    file.close()\n",
    "    return train_iter, test_iter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "#|  export\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "# from Political_Compass_AI.data_processing import return_iters\n",
    "# from Political_Compass_AI.data_processing import split_string\n",
    "from Political_Compass_AI.data_processing import yield_tokens\n",
    "from Political_Compass_AI.data_processing import collate_batch\n",
    "from Political_Compass_AI.model import TextClassificationModel\n",
    "from Political_Compass_AI.training import train\n",
    "from Political_Compass_AI.training import evaluate\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def collate_batch(\n",
    "        batch\n",
    "):\n",
    "    global text_pipeline\n",
    "    global db\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    label_pipeline = lambda x: int(x) - 1\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "def run(\n",
    "    _db:str # dn path to run alignment distribution\n",
    "    ,emsize = 128\n",
    "    ,LR = 5\n",
    "    ,BATCH_SIZE = 32\n",
    "    ,optimizer = \"Adagrad\"\n",
    "    ,EPOCHS = 20\n",
    "\n",
    "\n",
    "):\n",
    "    global text_pipeline\n",
    "    global db\n",
    "    db=_db\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "    label_pipeline = lambda x: int(x) - 1\n",
    "    train_iter, test_iter = return_iters(db)\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    train_iter, test_iter = return_iters(db)\n",
    "    dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "    train_iter, test_iter = return_iters(db)\n",
    "    _num_class = len(set([label for (label, text) in train_iter]))\n",
    "    print(_num_class)\n",
    "    num_class = 9\n",
    "    vocab_size = len(vocab)\n",
    "    model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "    run_ledger = open(\"Run_Ledger.txt\", 'a')\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if optimizer==\"Adagrad\":\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=LR)\n",
    "    elif optimizer==\"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "    else:\n",
    "        print(\"Choose a different optimizer\")\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "    total_accu = None\n",
    "    train_iter, test_iter = return_iters(db)\n",
    "    train_dataset = to_map_style_dataset(train_iter)\n",
    "    test_dataset = to_map_style_dataset(test_iter)\n",
    "    num_train = int(len(train_dataset) * 0.95)\n",
    "    split_train_, split_valid_ = \\\n",
    "        random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "    train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True, collate_fn=collate_batch)\n",
    "    valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True, collate_fn=collate_batch)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True, collate_fn=collate_batch)\n",
    "    first_flag = True\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_dataloader, model, optimizer, epoch)\n",
    "        accu_val = evaluate(valid_dataloader, model)\n",
    "        if total_accu is not None and total_accu > accu_val:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            total_accu = accu_val\n",
    "        if first_flag:\n",
    "            run_ledger.write(\"Database file: \" + db + \"\\t\" + \"Epochs:\" + str(EPOCHS) + \"\\t\" + \"LR: \" + str(\n",
    "                LR) + \"\\t\" + \"Batch Size: \" + str(BATCH_SIZE) + \"\\tinit accu_val:\" + str(accu_val) + \"\\n\")\n",
    "            first_flag = False\n",
    "        print('-' * 59)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "              'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                               time.time() - epoch_start_time,\n",
    "                                               accu_val))\n",
    "        print('-' * 59)\n",
    "    run_ledger.write(\"Final accu:\\t\" + str(accu_val) + \"\\n\\n\")\n",
    "    accu_test = evaluate(test_dataloader,model)\n",
    "    out = 'test accuracy {:8.3f}'.format(accu_test)\n",
    "    print(out)\n",
    "    run_ledger.close()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "7\n",
      "| epoch   1 |    50/  677 batches | accuracy    0.159\n",
      "| epoch   1 |   100/  677 batches | accuracy    0.117\n",
      "| epoch   1 |   150/  677 batches | accuracy    0.185\n",
      "| epoch   1 |   200/  677 batches | accuracy    0.125\n",
      "| epoch   1 |   250/  677 batches | accuracy    0.175\n",
      "| epoch   1 |   300/  677 batches | accuracy    0.155\n",
      "| epoch   1 |   350/  677 batches | accuracy    0.160\n",
      "| epoch   1 |   400/  677 batches | accuracy    0.150\n",
      "| epoch   1 |   450/  677 batches | accuracy    0.142\n",
      "| epoch   1 |   500/  677 batches | accuracy    0.130\n",
      "| epoch   1 |   550/  677 batches | accuracy    0.142\n",
      "| epoch   1 |   600/  677 batches | accuracy    0.165\n",
      "| epoch   1 |   650/  677 batches | accuracy    0.142\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  1.51s | valid accuracy    0.133 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |    50/  677 batches | accuracy    0.260\n",
      "| epoch   2 |   100/  677 batches | accuracy    0.230\n",
      "| epoch   2 |   150/  677 batches | accuracy    0.258\n",
      "| epoch   2 |   200/  677 batches | accuracy    0.275\n",
      "| epoch   2 |   250/  677 batches | accuracy    0.268\n",
      "| epoch   2 |   300/  677 batches | accuracy    0.253\n",
      "| epoch   2 |   350/  677 batches | accuracy    0.268\n",
      "| epoch   2 |   400/  677 batches | accuracy    0.255\n",
      "| epoch   2 |   450/  677 batches | accuracy    0.273\n",
      "| epoch   2 |   500/  677 batches | accuracy    0.278\n",
      "| epoch   2 |   550/  677 batches | accuracy    0.265\n",
      "| epoch   2 |   600/  677 batches | accuracy    0.285\n",
      "| epoch   2 |   650/  677 batches | accuracy    0.287\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  1.33s | valid accuracy    0.175 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |    50/  677 batches | accuracy    0.471\n",
      "| epoch   3 |   100/  677 batches | accuracy    0.443\n",
      "| epoch   3 |   150/  677 batches | accuracy    0.482\n",
      "| epoch   3 |   200/  677 batches | accuracy    0.450\n",
      "| epoch   3 |   250/  677 batches | accuracy    0.468\n",
      "| epoch   3 |   300/  677 batches | accuracy    0.448\n",
      "| epoch   3 |   350/  677 batches | accuracy    0.430\n",
      "| epoch   3 |   400/  677 batches | accuracy    0.463\n",
      "| epoch   3 |   450/  677 batches | accuracy    0.460\n",
      "| epoch   3 |   500/  677 batches | accuracy    0.468\n",
      "| epoch   3 |   550/  677 batches | accuracy    0.410\n",
      "| epoch   3 |   600/  677 batches | accuracy    0.440\n",
      "| epoch   3 |   650/  677 batches | accuracy    0.450\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  1.38s | valid accuracy    0.207 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |    50/  677 batches | accuracy    0.603\n",
      "| epoch   4 |   100/  677 batches | accuracy    0.573\n",
      "| epoch   4 |   150/  677 batches | accuracy    0.637\n",
      "| epoch   4 |   200/  677 batches | accuracy    0.600\n",
      "| epoch   4 |   250/  677 batches | accuracy    0.573\n",
      "| epoch   4 |   300/  677 batches | accuracy    0.545\n",
      "| epoch   4 |   350/  677 batches | accuracy    0.545\n",
      "| epoch   4 |   400/  677 batches | accuracy    0.520\n",
      "| epoch   4 |   450/  677 batches | accuracy    0.600\n",
      "| epoch   4 |   500/  677 batches | accuracy    0.605\n",
      "| epoch   4 |   550/  677 batches | accuracy    0.532\n",
      "| epoch   4 |   600/  677 batches | accuracy    0.542\n",
      "| epoch   4 |   650/  677 batches | accuracy    0.568\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  1.43s | valid accuracy    0.161 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |    50/  677 batches | accuracy    0.627\n",
      "| epoch   5 |   100/  677 batches | accuracy    0.725\n",
      "| epoch   5 |   150/  677 batches | accuracy    0.705\n",
      "| epoch   5 |   200/  677 batches | accuracy    0.705\n",
      "| epoch   5 |   250/  677 batches | accuracy    0.720\n",
      "| epoch   5 |   300/  677 batches | accuracy    0.743\n",
      "| epoch   5 |   350/  677 batches | accuracy    0.743\n",
      "| epoch   5 |   400/  677 batches | accuracy    0.777\n",
      "| epoch   5 |   450/  677 batches | accuracy    0.757\n",
      "| epoch   5 |   500/  677 batches | accuracy    0.708\n",
      "| epoch   5 |   550/  677 batches | accuracy    0.777\n",
      "| epoch   5 |   600/  677 batches | accuracy    0.725\n",
      "| epoch   5 |   650/  677 batches | accuracy    0.752\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  1.49s | valid accuracy    0.186 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |    50/  677 batches | accuracy    0.811\n",
      "| epoch   6 |   100/  677 batches | accuracy    0.820\n",
      "| epoch   6 |   150/  677 batches | accuracy    0.823\n",
      "| epoch   6 |   200/  677 batches | accuracy    0.825\n",
      "| epoch   6 |   250/  677 batches | accuracy    0.843\n",
      "| epoch   6 |   300/  677 batches | accuracy    0.797\n",
      "| epoch   6 |   350/  677 batches | accuracy    0.777\n",
      "| epoch   6 |   400/  677 batches | accuracy    0.830\n",
      "| epoch   6 |   450/  677 batches | accuracy    0.795\n",
      "| epoch   6 |   500/  677 batches | accuracy    0.792\n",
      "| epoch   6 |   550/  677 batches | accuracy    0.833\n",
      "| epoch   6 |   600/  677 batches | accuracy    0.802\n",
      "| epoch   6 |   650/  677 batches | accuracy    0.800\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  1.45s | valid accuracy    0.175 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |    50/  677 batches | accuracy    0.855\n",
      "| epoch   7 |   100/  677 batches | accuracy    0.780\n",
      "| epoch   7 |   150/  677 batches | accuracy    0.828\n",
      "| epoch   7 |   200/  677 batches | accuracy    0.807\n",
      "| epoch   7 |   250/  677 batches | accuracy    0.820\n",
      "| epoch   7 |   300/  677 batches | accuracy    0.812\n",
      "| epoch   7 |   350/  677 batches | accuracy    0.855\n",
      "| epoch   7 |   400/  677 batches | accuracy    0.777\n",
      "| epoch   7 |   450/  677 batches | accuracy    0.815\n",
      "| epoch   7 |   500/  677 batches | accuracy    0.820\n",
      "| epoch   7 |   550/  677 batches | accuracy    0.818\n",
      "| epoch   7 |   600/  677 batches | accuracy    0.845\n",
      "| epoch   7 |   650/  677 batches | accuracy    0.845\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.17s | valid accuracy    0.175 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |    50/  677 batches | accuracy    0.846\n",
      "| epoch   8 |   100/  677 batches | accuracy    0.815\n",
      "| epoch   8 |   150/  677 batches | accuracy    0.802\n",
      "| epoch   8 |   200/  677 batches | accuracy    0.835\n",
      "| epoch   8 |   250/  677 batches | accuracy    0.838\n",
      "| epoch   8 |   300/  677 batches | accuracy    0.833\n"
     ]
    }
   ],
   "source": [
    "model =run(\"../9_aligns_merged\",BATCH_SIZE=8)\n",
    "#torch.save(model.state_dict(), <path_to>)\n",
    "# model.load_state_dict(torch.load(<path_to>))\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "mapping = {\n",
    "        1:\"Libertarian Left\",\n",
    "        2: \"Libertarian Right\",\n",
    "        3:\"Authoritarian Left\" ,\n",
    "        4: \"Authoritarian Right\",\n",
    "        5: \"Centrist\",\n",
    "         6:\"Authoritarian Center\",\n",
    "        7: \"Left\",\n",
    "        8: \"Right\",\n",
    "        9:\"Libertarian Center\" ,\n",
    "    }\n",
    "ex_text_str = \"\"\"\n",
    "considering Ukraine already has been striking airfields in Russia, not sure what any of the weapons promised in this round would change\n",
    "just posturing as always\n",
    "the Kremlin is well aware that they have no chance of obtaining a strategic advantage as long as the west continues to supply Ukraine, so they're talking big and trying to cut the supply that way\n",
    "it is and always has been a bully going \"no stop it you weren't supposed to fight back >:(\"\n",
    "\"\"\"\n",
    "print(\"This is a %s comment\" % mapping[predict(ex_text_str, text_pipeline)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-54-ec0cc53bb288>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[0mit\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0malways\u001B[0m \u001B[0mhas\u001B[0m \u001B[0mbeen\u001B[0m \u001B[0ma\u001B[0m \u001B[0mbully\u001B[0m \u001B[0mgoing\u001B[0m \u001B[1;34m\"no stop it you weren't supposed to fight back >:(\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m \"\"\"\n\u001B[1;32m---> 27\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"This is a %s comment\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mex_text_str\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext_pipeline\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-54-ec0cc53bb288>\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(text, text_pipeline)\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m         \u001B[0mtext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext_pipeline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m         \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# https://old.reddit.com/r/PoliticalCompassMemes/comments/x774os/conservative_you_say_sounds_fine_to_me/inbbz52/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
