# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/hf/hf.ipynb.

# %% auto 0
__all__ = ['token', 'dataset', 'label2id', 'id2label', 'tokenizer', 'tokenized_dataset', 'data_collator', 'accuracy', 'model',
           'logger', 'log_level', 'tracing_steps', 'tracing', 'training_args', 'trainer', 'preprocess_function',
           'compute_metrics']

# %% ../nbs/hf/hf.ipynb 0
import torch.cuda
from huggingface_hub import notebook_login

# %% ../nbs/hf/hf.ipynb 2
token = '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'
import subprocess
subprocess.run(["huggingface-cli", "login", "--token", token])

# %% ../nbs/hf/hf.ipynb 3
from datasets import load_dataset
dataset = load_dataset("csv",data_files={'test':["_test.csv"],'train':['_train.csv']},encoding='latin1')

# %% ../nbs/hf/hf.ipynb 4
label2id = {
    "Libertarian Left": 0,
    "Libertarian Right": 1,
    "Authoritarian Left": 2,
    "Authoritarian Right": 3,
    "Centrist": 4,
    "Authoritarian Center": 5,
    "Left": 6,
    "Right": 7,
    "Libertarian Center": 8,
}
id2label = { 0:"Libertarian Left",
     1:"Libertarian Right",
    2:"Authoritarian Left",
     3:"Authoritarian Right",
     4:"Centrist",
     5:"Authoritarian Center",
     6:"Left",
    7:"Right",
     8:"Libertarian Center",}

# %% ../nbs/hf/hf.ipynb 5
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True,padding=True,max_length=512)
tokenized_dataset = dataset.map(preprocess_function,batched=True)
tokenized_dataset.set_format("torch")

# %% ../nbs/hf/hf.ipynb 10
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# %% ../nbs/hf/hf.ipynb 11
import evaluate

accuracy = evaluate.load("accuracy")

# %% ../nbs/hf/hf.ipynb 12
import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    print(predictions)
    return accuracy.compute(predictions=predictions, references=labels)

# %% ../nbs/hf/hf.ipynb 14
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=9, id2label=id2label, label2id=label2id
).to("cuda")

# %% ../nbs/hf/hf.ipynb 16
import logging
import sys
logger = logging.getLogger(__name__)
import datasets
import transformers
# Setup logging
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)],
)

# set the main code and the modules it uses to the same log-level according to the node
log_level = "DEBUG"
logger.setLevel(log_level)
datasets.utils.logging.set_verbosity(log_level)
transformers.utils.logging.set_verbosity(log_level)
from torch import nn
from transformers import Trainer
tracing_steps= 1
tracing="steps"
# class RegressionTrainer(Trainer):
#     def compute_loss(self, model, inputs, return_outputs=False):
#         labels = inputs.get("labels")
#         print(labels)
#         outputs = model(**inputs)
#         logits = outputs.get('logits')
#         loss_fct = nn.BCEWithLogitsLoss()
#         loss = loss_fct(logits.squeeze(), labels.squeeze())
#         return (loss, outputs) if return_outputs else loss
training_args = TrainingArguments(
    output_dir="pcm_model",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=4,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
    log_level="debug",
    logging_steps=tracing_steps, # to get more information to TB
    eval_steps=tracing_steps,
    save_steps=tracing_steps,
auto_find_batch_size =True,
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,

)

# trainer = RegressionTrainer(
#     model=model,
#     args=training_args,
#     train_dataset=tokenized_dataset["train"],
#     eval_dataset=tokenized_dataset["test"],
#     tokenizer=tokenizer,
#     data_collator=data_collator,
#     compute_metrics=compute_metrics,
#
# )

import torch
print(torch.cuda.is_available())
trainer.train()
trainer.push_to_hub()
file = open("pcm_model_eval.txt", "w")
file.write(str(trainer.evaluate(eval_dataset=tokenized_dataset["test"])))
file.close()